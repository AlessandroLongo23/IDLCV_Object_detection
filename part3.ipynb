{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_208357/2579793710.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/arth/Desktop/various/CV/poster3/proposal_classifier_NMS_SS.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProposalClassifierCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=200704, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import import_ipynb\n",
    "\n",
    "# Load your model architecture\n",
    "from CNN_proposals_2 import ProposalClassifierCNN, ProposalClassifierResNet, ProposalClassifierVGG16   # Replace with your model class\n",
    "\n",
    "# Load the trained model\n",
    "#model = ProposalClassifierVGG16(2)\n",
    "#model = ProposalClassifierResNet(2)\n",
    "model = ProposalClassifierCNN(2)\n",
    "model.load_state_dict(torch.load('/home/arth/Desktop/various/CV/poster3/proposal_classifier_NMS_SS.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the splits.json\n",
    "with open('/home/arth/Desktop/various/CV/poster3/Potholes/Potholes/splits.json', 'r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "test_images_xml = splits['test']  # List of test image XML filenames\n",
    "\n",
    "# Convert XML filenames to image filenames\n",
    "test_image_filenames = [filename.replace('.xml', '.jpg') for filename in test_images_xml]\n",
    "\n",
    "# Define directories\n",
    "image_dir = '/home/arth/Desktop/various/CV/poster3/Potholes/Potholes/annotated-images'      # Replace with your image directory\n",
    "proposal_dir = '/home/arth/Desktop/various/CV/poster3/Potholes/Potholes/NMS/Labeled_Proposals_SS'  # Replace with your proposals directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image transformatiions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.Pad((0, 0, 256, 256), fill=0, padding_mode='constant'),  # Adjust padding as needed\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import logging\n",
    "\n",
    "def parse_voc_xml(xml_file):\n",
    "    \"\"\"\n",
    "    Parse PASCAL VOC XML file to extract ground truth bounding boxes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "    except ET.ParseError as e:\n",
    "        logging.error(f\"Error parsing XML file {xml_file}: {e}\")\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"XML file not found: {xml_file}\")\n",
    "        return []\n",
    "\n",
    "    root = tree.getroot()\n",
    "    boxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        bbox = obj.find('bndbox')\n",
    "        if bbox is None:\n",
    "            continue\n",
    "        try:\n",
    "            x1 = int(float(bbox.find('xmin').text))\n",
    "            y1 = int(float(bbox.find('ymin').text))\n",
    "            x2 = int(float(bbox.find('xmax').text))\n",
    "            y2 = int(float(bbox.find('ymax').text))\n",
    "            if x1 >= x2 or y1 >= y2:\n",
    "                logging.warning(f\"Invalid bounding box in {xml_file}: {bbox.text}\")\n",
    "                continue\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "        except (AttributeError, ValueError) as e:\n",
    "            logging.warning(f\"Malformed bounding box in {xml_file}: {e}\")\n",
    "            continue\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:01<00:00, 84.10it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "all_detections = []\n",
    "all_annotations = []\n",
    "\n",
    "for img_filename in tqdm(test_image_filenames):\n",
    "    # Load the image\n",
    "    img_path = os.path.join(image_dir, img_filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Load the proposals\n",
    "    proposal_filename = img_filename.replace('.jpg', '_labeled_proposals.npy')\n",
    "    proposal_path = os.path.join(proposal_dir, proposal_filename)\n",
    "    proposals = np.load(proposal_path, allow_pickle=True)  # Shape: (num_proposals, 4)\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    for proposal in proposals:\n",
    "    # Ensure proposal is a dictionary\n",
    "        if not isinstance(proposal, dict):\n",
    "            logging.warning(f\"Unexpected proposal format in {proposal_path}: {proposal}\")\n",
    "            continue\n",
    "\n",
    "        # Extract 'box' and 'label'\n",
    "        try:\n",
    "            box = proposal['box']\n",
    "            label = proposal['label']\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Missing key {e} in proposal: {proposal} in file {proposal_path}\")\n",
    "            continue\n",
    "\n",
    "        # Optionally, filter out background proposals\n",
    "        if label.lower() == 'background':\n",
    "            continue  # Skip background proposals\n",
    "\n",
    "        # Extract coordinates from the 'box' array\n",
    "        if len(box) != 4:\n",
    "            logging.warning(f\"Invalid box format in proposal: {proposal} in file {proposal_path}\")\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = box\n",
    "        try:\n",
    "            x1 = int(x)\n",
    "            y1 = int(y)\n",
    "            x2 = int(x + w)\n",
    "            y2 = int(y + h)\n",
    "        except (TypeError, ValueError) as e:\n",
    "            logging.warning(f\"Invalid box values in proposal: {proposal} in file {proposal_path}\")\n",
    "            continue\n",
    "\n",
    "        # Validate bounding box coordinates\n",
    "        if x1 >= x2 or y1 >= y2:\n",
    "            logging.warning(f\"Invalid bounding box coordinates: {proposal} in file {proposal_path}\")\n",
    "            continue\n",
    "\n",
    "        # Proceed with cropping and processing\n",
    "        try:\n",
    "            cropped_region = image.crop((x1, y1, x2, y2))\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error cropping image {img_filename} with bbox {proposal}: {e}\")\n",
    "            continue\n",
    "\n",
    "        input_tensor = preprocess(cropped_region).unsqueeze(0).to(device)\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            # Assuming the model outputs logits; adjust if necessary\n",
    "            if output.dim() == 1:\n",
    "                output = output.unsqueeze(0)\n",
    "            scores = torch.softmax(output, dim=1)\n",
    "            score, predicted_class = torch.max(scores, dim=1)\n",
    "            score = score.item()\n",
    "            predicted_class = predicted_class.item()\n",
    "\n",
    "        detections.append({\n",
    "            'bbox': [x1, y1, x2, y2],\n",
    "            'score': score,\n",
    "            'class': predicted_class\n",
    "        })\n",
    "\n",
    "    # Apply NMS (to be implemented in the next step)\n",
    "    # Store detections and annotations for evaluation\n",
    "    all_detections.append({'image_id': img_filename, 'detections': detections})\n",
    "\n",
    "    # Load ground truth annotations\n",
    "    xml_filename = img_filename.replace('.jpg', '.xml')\n",
    "    xml_path = os.path.join('/home/arth/Desktop/various/CV/poster3/Potholes/Potholes/annotated-images', xml_filename)\n",
    "    gt_boxes = parse_voc_xml(xml_path)  # Function to parse XML files (defined later)\n",
    "    all_annotations.append({'image_id': img_filename, 'bboxes': gt_boxes})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'box': array([289, 270,  45,  30]), 'label': 'background'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load a single proposal file\n",
    "proposal_path = '/home/arth/Desktop/various/CV/poster3/Potholes/Potholes/NMS/Labeled_Proposals_SS/img-529_labeled_proposals.npy'  # Replace with an actual file path\n",
    "proposals = np.load(proposal_path, allow_pickle=True)\n",
    "\n",
    "# Check the type of the first proposal\n",
    "first_proposal = proposals[0]\n",
    "print(type(first_proposal))\n",
    "print(first_proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, iou_threshold=0.5):\n",
    "    import numpy as np\n",
    "\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        inds = np.where(ovr <= iou_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the NMS to the Detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After collecting all detections\n",
    "boxes = np.array([d['bbox'] for d in detections])\n",
    "scores = np.array([d['score'] for d in detections])\n",
    "\n",
    "if len(boxes) > 0:\n",
    "    keep_indices = nms(boxes, scores, iou_threshold=0.5)\n",
    "    final_detections = [detections[i] for i in keep_indices]\n",
    "else:\n",
    "    final_detections = []\n",
    "\n",
    "# Update all_detections with NMS results\n",
    "all_detections[-1]['detections'] = final_detections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Object Detection with Average Precision(AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_voc_xml(xml_file):\n",
    "    tree=ET.åarse(xml_file)\n",
    "    root=tree.groot()\n",
    "    boxes=[]\n",
    "    for obj in root.findall('object'):\n",
    "        bbox= obj.find('bndbox')\n",
    "        x1=int(float(bbox.find('xmin').text))\n",
    "        y1=int(float(bbox.find('ymin').text))\n",
    "        x2=int(float(bbox.find('xmax').text))\n",
    "        y2=int(float(bbox.find('xymax').text))\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copmute Intersections over Union(IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    x1_max = max(box1[0], box2[0])\n",
    "    y1_max = max(box1[1], box2[1])\n",
    "    x2_min = min(box1[2], box2[2])\n",
    "    y2_min = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2_min - x1_max) * max(0, y2_min - y1_max)\n",
    "    if inter_area == 0:\n",
    "        return 0.0\n",
    "\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Detections and compute AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detections(all_detections, all_annotations, iou_threshold=0.8):\n",
    "    all_tp = []\n",
    "    all_fp = []\n",
    "    all_scores = []\n",
    "    num_gt_boxes = 0\n",
    "\n",
    "    # Create a mapping from image_id to ground truth boxes\n",
    "    gt_dict = {ann['image_id']: ann['bboxes'] for ann in all_annotations}\n",
    "\n",
    "    for image_info in all_detections:\n",
    "        image_id = image_info['image_id']\n",
    "        detections = image_info['detections']\n",
    "        gt_boxes = gt_dict.get(image_id, [])\n",
    "        num_gt_boxes += len(gt_boxes)\n",
    "        detected = [False] * len(gt_boxes)\n",
    "\n",
    "        for det in detections:\n",
    "            best_iou = 0.0\n",
    "            best_gt_idx = -1\n",
    "            for idx, gt_box in enumerate(gt_boxes):\n",
    "                iou = compute_iou(det['bbox'], gt_box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "            if best_iou >= iou_threshold and not detected[best_gt_idx]:\n",
    "                all_tp.append(1)\n",
    "                all_fp.append(0)\n",
    "                detected[best_gt_idx] = True\n",
    "            else:\n",
    "                all_tp.append(0)\n",
    "                all_fp.append(1)\n",
    "            all_scores.append(det['score'])\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_tp = np.array(all_tp)\n",
    "    all_fp = np.array(all_fp)\n",
    "    all_scores = np.array(all_scores)\n",
    "\n",
    "    # Sort by scores\n",
    "    sorted_indices = np.argsort(-all_scores)\n",
    "    all_tp = all_tp[sorted_indices]\n",
    "    all_fp = all_fp[sorted_indices]\n",
    "\n",
    "    # Cumulative sums\n",
    "    cum_tp = np.cumsum(all_tp)\n",
    "    cum_fp = np.cumsum(all_fp)\n",
    "\n",
    "    recalls = cum_tp / num_gt_boxes\n",
    "    precisions = cum_tp / (cum_tp + cum_fp)\n",
    "\n",
    "    # Compute AP using the trapezoidal rule\n",
    "    ap = np.trapz(precisions, recalls)\n",
    "\n",
    "    return ap, precisions, recalls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision (AP): 0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_208357/3117772641.py:52: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  ap = np.trapz(precisions, recalls)\n"
     ]
    }
   ],
   "source": [
    "ap, precisions, recalls = evaluate_detections(all_detections, all_annotations)\n",
    "print(f'Average Precision (AP): {ap:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
